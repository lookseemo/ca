---
title: "CA - S8: Example"
author: Josep Curto, IE Business School
abstract: "This document introduces how to calculate Customer Segmentation with R. In particular, with k-means."
keywords: "r, customer segmentation"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_notebook: 
    fig_caption: yes
    toc: yes
    toc_float: yes
    self_contained: yes
---

# Wholesale customers Data Set

We will consider the following data set: [Wholesale customers Data Set](Abreu, N. (2011). Analise do perfil do cliente Recheio e desenvolvimento de um sistema promocional. Mestrado em Marketing, ISCTE-IUL, Lisbon)
 
It contains the following attributes:

 - FRESH: annual spending (m.u.) on fresh products (Continuous); 
 - MILK: annual spending (m.u.) on milk products (Continuous); 
 - GROCERY: annual spending (m.u.) on grocery products (Continuous); 
 - FROZEN: annual spending (m.u.) on frozen products (Continuous) 
 - DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) 
 - DELICATESSEN: annual spending (m.u.) on and delicatessen products (Continuous); 
 - CHANNEL: customers Channel - Horeca (Hotel/Restaurant/Caf√©) or Retail channel (Nominal) 
 - REGION: customers Region of Lisbon, Oporto or Other (Nominal) 

# Calculate Customer Segmentation with R

## Load packages

```{r packages, warning=FALSE, echo=FALSE, message=FALSE}
# Cleaning the environment
rm(list=ls())

# List of packages for session
.packages = c("ggplot2", "NbClust","ggfortify","GGally","cluster","factoextra","clustertend")

# Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])

# Load packages into session 
lapply(.packages, suppressPackageStartupMessages(require), character.only=TRUE)
```

## Loading data

We need a seed to be able to reproduce our analysis:

```{r}
set.seed(1234)
```

First, we load the data:

```{r loading data}
data <- read.csv('data/s8.csv', header = T,sep=',')
```

Let's try to understant our data set. Fist the structure:

```{r structure}
str(data)
```

Second the main statistics:

```{r summary}
summary(data)
```

And now the correlation between different variables:

```{r correlation}
ggcorr(data, label = TRUE, label_size = 4, label_round = 2, size = 2)
```

**Question: What we can observe?**

# Hopkins Statistic

We need to statistically evaluate clustering tendency:

```{r}
hopkins(data,n = nrow(data)-1)
```

# PCA

Do we have too many variables? We use a technique call Principal Component Analysis. 

**Principal component analysis** (or PCA), is a linear transformation of the data which looks for the axis where the data has the most variance. PCA will create new variables which are linear combinations of the original ones, these new variables will be orthogonal (i.e. correlation equals to zero). PCA can be seen as a rotation of the initial space to find more suitable axis to express the variability in the data.
On the new variables have been created, you can select the most important ones. The threshold is up-to-you and depends on how much variance you want to keep.

*Note*: Since PCA is linear, it mostly works on linearly separable data. Hence, if you want to perform classification on non-linear data, linear PCA will probably make you lose a lot of information.
There is another technique called kernel PCA that can work on nonlinearly separable data.

What if we apply PCA to the whole data set:

```{r pca}
pca <- prcomp(data, scale=TRUE)
pca
```

```{r pca summary}
summary(pca)
```

How many (pc) variables do we need to explain our data set:

```{r pca plot}
screeplot(pca, type="lines",col=3)
```

```{r pc1 vs pc2}
autoplot(pca, data = data)
```

Let's add some color:

```{r}
autoplot(pca, data = data, colour = 'Channel')
```

One more graph (with eigenvalues):

```{r}
autoplot(pca, data = data, colour = 'Channel', loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
```

# Customer Segmentation

We need to scale the data to compare diferent magnitudes. This is call data normalization.

```{r}
df <- scale(data) 
```

We will use Kmeans for our analysis. We need to determine the right number of clusters. 

**[Option 1]** Elbow method. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.

```{r}
fviz_nbclust(df, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)
```

Three clusters are suggested.

**[Option 2]** Average silhouette method. Briefly, it measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The location of the maximum is considered as the appropriate number of clusters.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```

Five clusters are suggested.

**[Option 3]** Nbclust. It provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

```{r}
res <- NbClust(df, diss=NULL, distance = "euclidean", min.nc=2, max.nc=12, 
             method = "kmeans", index = "all")

fviz_nbclust(res) + theme_minimal()
```

Now we can create 2, 3 or 4 clusters. Let's create three:

```{r}
fit <- kmeans(df, 3)
```

What the fit object contains?

```{r}
fit$centers
fit$size
```

We need to understand the new groups using the average:

```{r}
clusters <- aggregate(data,by=list(fit$cluster),FUN=mean)
clusters
```

We can add back the segmentation

```{r}
data <- data.frame(data, fit$cluster)
head(data)
```

Let's make a plot:

```{r}
ggplot(data, aes(Fresh, Delicassen, color = as.factor(fit$cluster))) + geom_point()
```

# Question 1: What happens if we consider 4 clusters? 

# Question 2: What happens if we make the same analysis without the columns channel and region?

Consider the following data frame:

```{r}
df <- data[-(1:2)]
df
```
